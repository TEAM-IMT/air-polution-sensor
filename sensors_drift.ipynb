{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-rhep67R-Pe"
   },
   "source": [
    "[#air-polution-sensor](https://github.com/Johansmm/air-polution-sensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_YQLSc_IHJc"
   },
   "source": [
    " # Spatial and temporal analysis of signals to detect drift of air quality sensors\n",
    " \n",
    "The network of sensors allows to follow a complex phenomenon by observing the temporal evolution of the values in several points, and by crossing the information from one sensor to another. This allows, for example, to carry out a meteorological or seismic monitoring, by detecting a cloud or a tremor. However, the sensors used are sometimes subject to drift, wear and tear or possible interference, which makes some of the observed values false. It is therefore important to be able to determine if a sensor starts to drift in order to allow a good analysis of the data. Therefore, the objective of this project is to detect a drifting sensor (when and where) in a real sensor's network. For this, we will explore the usefulness of a particular mathematical tool: the graph space-time spectrogram. Such a tool allows to decompose a space-time series into a sum of space-time frequencies, in a similar way to Fourier analysis, but for signals in graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1016,
     "status": "ok",
     "timestamp": 1615554897809,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "VUcPLCl2mTts"
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# Basis libraries \n",
    "import sys, getpass, os, copy, tqdm\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "from matplotlib import rc\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "import scipy.signal\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.transforms as mtransforms\n",
    "from statistics import mean \n",
    "from IPython.display import Image\n",
    "\n",
    "# Library for boxplots\n",
    "# font = {'family':'sans-serif','sans-serif':['Helvetica'], 'size':18}\n",
    "# rc('font', **font)\n",
    "# rc('text', usetex=True)\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 503,
     "status": "ok",
     "timestamp": 1615554901939,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "Cv4wvg9oDvu2",
    "outputId": "1386297d-3543-463f-c15d-7ed193ead4f5"
   },
   "outputs": [],
   "source": [
    "# Colab access\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    from google.colab import files\n",
    "    drive.mount('/content/gdrive')\n",
    "\n",
    "# If you work-directory don't match, please add the new path to pwd_list\n",
    "cwd_list = [\"/content/gdrive/My Drive/S5 Project: Air polution/air-polution-sensor/\",\n",
    "            \"/content/gdrive/My Drive/Colab Notebooks/ProyectS5/air-polution-sensor/\",\n",
    "            \"/content/gdrive/MyDrive/IMT Atlantique/Project_S5_Air/air-polution-sensor/\",\n",
    "            os.path.join(os.getcwd(),\"air-polution-sensor\")]\n",
    "\n",
    "for cwd in cwd_list:\n",
    "    if os.path.isdir(cwd):\n",
    "        %cd \"$cwd\"\n",
    "        break\n",
    "if os.path.split(os.getcwd())[-1] != \"air-polution-sensor\":\n",
    "    print(\"[WARNING]: Incorrect working directory. Please add the fix-directory to the cwd_list.\")\n",
    "else: \n",
    "    print(\"[INFO]: Work directory: \" + os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTMEWxUCIHJj"
   },
   "source": [
    "## **Download database**\n",
    "Initially, the project [air-polution-sensor](https://github.com/Johansmm/air-polution-sensor.git) contains some default databases, which must be decompressed. Additionally, a larger database should be downloaded. This process is presented below.\n",
    "\n",
    "Note: Make sure you are inside the repository folder [air-polution-sensor](https://github.com/Johansmm/air-polution-sensor.git)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1423,
     "status": "ok",
     "timestamp": 1615554856124,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "M_ArY6OPIHJk",
    "outputId": "f413b888-7ec1-4c91-be48-38944fe40882"
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"./data/origin_data\"):\n",
    "    if not os.path.isdir(\"./data\"): os.mkdir(\"./data\")\n",
    "    if not os.path.isdir(\"./data/origin_data\"): os.mkdir(\"./data/origin_data\")\n",
    "    !wget --output-document=\"./data/data.zip\" \"https://www.dropbox.com/sh/w1704rg9fd9z4pq/AABG3YCUMHTwbFhf3pKgti-Qa/PuneData_UseThis/August2019?dl=0&subfolder_nav_tracking=1\"\n",
    "    !unzip \"./data/data.zip\" -d \"./data/origin_data\"\n",
    "\n",
    "if not os.path.isfile(\"./data/super_df.csv\") and os.path.isfile(\"./data/super_df.rar\"):\n",
    "    !unrar x \"./data/super_df.rar\" \"./data/\"\n",
    "    \n",
    "if not os.path.isfile(\"./data/drift_joint_drift.csv\") and os.path.isfile(\"./data/drift_joint_drift.rar\"):\n",
    "    !unrar x \"./data/drift_joint_drift.rar\" \"./data/\"\n",
    "    \n",
    "if not os.path.isfile(\"./data/drift_joint_real.csv\") and os.path.isfile(\"./data/drift_joint_real.rar\"):\n",
    "    !unrar x \"./data/drift_joint_real.rar\" \"./data/\"\n",
    "\n",
    "clear_output(wait = True)\n",
    "print(\"[INFO]: Data downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11509,
     "status": "ok",
     "timestamp": 1615554867352,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "B9FmgEdBIUxB",
    "outputId": "2dfc43e1-8d5a-4b1c-961d-4fef5a2a0aad"
   },
   "outputs": [],
   "source": [
    "# Specific libraries\n",
    "#!pip install -r requirements.txt\n",
    "#!pip install -U scipy\n",
    "from libraries.global_functions import *\n",
    "from libraries.animation_utils import *\n",
    "np.random.seed(0)\n",
    "\n",
    "clear_output(wait = True)\n",
    "print(\"[INFO]: Successfully loaded libraries!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21262,
     "status": "ok",
     "timestamp": 1615554880075,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "NX6lDNDBwcBU",
    "outputId": "e198f4e8-d893-41e7-abb9-0adbf851e497"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !sudo apt-get install python3-dev graphviz libgraphviz-dev pkg-config\n",
    "    clear_output(wait = True)\n",
    "else:\n",
    "    print(\"[INFO]: If you have problems, remember execute the following code:\\n>> sudo apt-get install python3-dev graphviz libgraphviz-dev pkg-config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mA0MkNvPHzr-"
   },
   "source": [
    "## **Handling of NAN Values**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNwTanN3IS57"
   },
   "source": [
    "In order to obtain a good representation in the spectrogram is necessary to tame the NAN values that have the data provided by the sensors, for this reason we used a python library called fancyimpute that is very useful to perform data imputation, with different algorithms such as matrix completion, matrix completion by iterative low-rank SVD decomposition or nuclear norm minimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "executionInfo": {
     "elapsed": 70618,
     "status": "ok",
     "timestamp": 1615547198309,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "f4aFdmFDFUhT",
    "outputId": "f957e65f-b948-46ed-c077-1eab17a2c9b3"
   },
   "outputs": [],
   "source": [
    "Image(\"./results/tensor.png\", width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxpIqD7gIFDV"
   },
   "source": [
    "### Test in interval of the time series with fancyimpute\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5EfHeNudKVSd"
   },
   "source": [
    "First, the dataframe is read where all the sensor data is contained in the time intervals that are interesting to observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 531,
     "status": "ok",
     "timestamp": 1615559415194,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "oBfDBsDOKGwi"
   },
   "outputs": [],
   "source": [
    "super_df = pd.read_csv('./data/dfv3.csv', header=0, index_col=0)\n",
    "#super_df = pd.read_csv('./data/superdfi.csv', header=0, index_col=0)\n",
    "aux = super_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TX6E6qtNvtH"
   },
   "source": [
    "A dataframe sample is taken from the data of three sensors and the NAN values are removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1JgqzAG-AS_"
   },
   "source": [
    "Due to the fact that the original data has NAN values, for the performance evaluation experiment these values will be excluded to pass a matrix without null values through the models and to be able to apply the performance indicators (MAE, MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 648,
     "status": "ok",
     "timestamp": 1615559416481,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "XdP3ClxeuwEZ"
   },
   "outputs": [],
   "source": [
    "aux = pd.pivot_table(data=aux,values='PM2_MOY',index='Sensor',columns= ['Date', 'Hour'])\n",
    "#aux = aux.fillna(method='bfill')\n",
    "aux = aux.dropna(axis= 1)\n",
    "columnst = aux.columns\n",
    "indext = aux.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 923,
     "status": "ok",
     "timestamp": 1615559418273,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "dtyUqcHuFtsL",
    "outputId": "882125c2-0255-40dd-e721-1ee79badc60c"
   },
   "outputs": [],
   "source": [
    "aux.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 527,
     "status": "ok",
     "timestamp": 1615559420337,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "Ao6Xfsx5-kjU",
    "outputId": "299e3be5-4e14-4820-c5f6-3ffbc59a26cd"
   },
   "outputs": [],
   "source": [
    "aux= aux.T.reset_index(drop=True).T\n",
    "aux.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1179,
     "status": "ok",
     "timestamp": 1615555242617,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "IJIfrmdWlHU4",
    "outputId": "95da1ce9-77be-4e3c-eef9-8bd9eb7290b1"
   },
   "outputs": [],
   "source": [
    "aux.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 776,
     "status": "ok",
     "timestamp": 1615555242828,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "Qtq6ME6eF-f1",
    "outputId": "41ef3233-a534-4a21-88fc-b01d26d69cc1"
   },
   "outputs": [],
   "source": [
    "super_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aIFGF5iHOfiB"
   },
   "source": [
    "We transform the df into a numpy vector and randomly replace existing values with NAN values, we also create a mask of the positions of these values for later comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 691,
     "status": "ok",
     "timestamp": 1615555245089,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "mZm228uZOcC1",
    "outputId": "59305003-24ce-4d7f-8074-df0385406711"
   },
   "outputs": [],
   "source": [
    "f = 25 # window of the time series\n",
    "outTensor = np.array([]).reshape(aux.shape[0], 0, f)\n",
    "\n",
    "for t in range(0, aux.shape[1], f):\n",
    "  auxTenso = aux.values[:,t:t+f][:, None]\n",
    "  auxTenso = np.pad(auxTenso, ((0,0),(0,0),(0,f-auxTenso.shape[2])), mode='constant')\n",
    "  outTensor = np.append(outTensor, auxTenso, axis = 1)\n",
    "print(outTensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djqKbjTOGT-Q"
   },
   "source": [
    "Percentage of NAN values in the Tensor 23%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 830,
     "status": "ok",
     "timestamp": 1615555246863,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "hAB04Q-0GS68",
    "outputId": "9f94c194-deb8-465b-82e7-a6333907b4ea"
   },
   "outputs": [],
   "source": [
    "np.isnan(outTensor).sum()/np.prod(outTensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CD8jG37W1sA7"
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 507,
     "status": "ok",
     "timestamp": 1615555250694,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "Jj8Od2jbGaMg"
   },
   "outputs": [],
   "source": [
    "def plotTs(actual, imputed, title):\n",
    "  plt.figure(figsize=(20,10))\n",
    "  plt.plot(actual, 'r', label='Actual', linewidth=0.8)\n",
    "  plt.plot(imputed, 'b', label='Imputated', linewidth=0.8 )\n",
    "  plt.title(title, size = 20)\n",
    "  plt.xlabel('Time',size=18)\n",
    "  plt.ylabel('Average PM2.5',size=18)\n",
    "  plt.legend(fontsize=20)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 753,
     "status": "ok",
     "timestamp": 1615555250960,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "PtfEeLk86DCY"
   },
   "outputs": [],
   "source": [
    "def plotTS (observed_data, imputed_data, sensor, title):\n",
    "\n",
    "  s = sensor.split('.')[0]\n",
    "  s=s[0:8] if len(s) == 10 else s[0:7] \n",
    "  plt.figure(figsize=(20,10))\n",
    "  plt.subplot(211)\n",
    "  plt.plot(observed_data[sensor],'r')\n",
    "  plt.title('Original data, ' + s, size=20)\n",
    "  plt.xlabel('Time',size=18)\n",
    "  plt.ylabel('Average PM2.5',size=18)\n",
    "  plt.subplots_adjust(hspace=0.4)\n",
    "  \n",
    "  #plt.figure(figsize=(20,5))\n",
    "  plt.subplot(212)\n",
    "  plt.plot(imputed_data[sensor],'b')\n",
    "  plt.title(title + ', ' + s, size=20)\n",
    "  plt.xlabel('Time',size=18)\n",
    "  plt.ylabel('Average PM2.5',size=18)\n",
    "\n",
    "  plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 737,
     "status": "ok",
     "timestamp": 1615555250961,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "J7P-yi0L1lK_"
   },
   "outputs": [],
   "source": [
    "def scatter(x,y,title):\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.scatter(x, y, c='blue')\n",
    "  line = mlines.Line2D([0, 1], [0, 1], color='black')\n",
    "  transform = ax.transAxes\n",
    "  line.set_transform(transform)\n",
    "  ax.add_line(line)\n",
    "  plt.title(title)\n",
    "  plt.xlabel('Observed value')\n",
    "  plt.ylabel('Imputed value')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtgyNwz7ie6O"
   },
   "source": [
    "### Bayesian Probabilistic Matrix Factorization (BPMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "executionInfo": {
     "elapsed": 11467,
     "status": "ok",
     "timestamp": 1615554842612,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "gq7eiD8AHX3O",
    "outputId": "f789b7e2-9b7d-43c4-f66c-3c1fc9a1de80"
   },
   "outputs": [],
   "source": [
    "!pip install -U scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1077,
     "status": "ok",
     "timestamp": 1615555257374,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "OKLOswRjiiff"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv as inv\n",
    "from numpy.random import normal as normrnd\n",
    "from scipy.linalg import khatri_rao as kr_prod\n",
    "from scipy.stats import wishart\n",
    "from numpy.linalg import solve as solve\n",
    "from scipy.linalg import cholesky as cholesky_upper\n",
    "from scipy.linalg import solve_triangular as solve_ut\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1495,
     "status": "ok",
     "timestamp": 1615555258192,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "xktEADw9kRqg"
   },
   "outputs": [],
   "source": [
    "def mvnrnd_pre(mu, Lambda):\n",
    "    src = normrnd(size = (mu.shape[0],))\n",
    "    return solve_ut(cholesky_upper(Lambda, overwrite_a = True, check_finite = False), \n",
    "                    src, lower = False, check_finite = False, overwrite_b = True) + mu\n",
    "\n",
    "def cov_mat(mat, mat_bar):\n",
    "    mat = mat - mat_bar\n",
    "    return mat.T @ mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWrtu2r0I-6C"
   },
   "source": [
    "Sample factor $\\boldsymbol{W}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1359,
     "status": "ok",
     "timestamp": 1615555259046,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "rDAzqvp1kUjE"
   },
   "outputs": [],
   "source": [
    "def sample_factor_w(tau_sparse_mat, tau_ind, W, X, tau, beta0 = 1, vargin = 0):\n",
    "    \"\"\"Sampling N-by-R factor matrix W and its hyperparameters (mu_w, Lambda_w).\"\"\"\n",
    "    \n",
    "    dim1, rank = W.shape\n",
    "    W_bar = np.mean(W, axis = 0)\n",
    "    temp = dim1 / (dim1 + beta0)\n",
    "    var_mu_hyper = temp * W_bar\n",
    "    var_W_hyper = inv(np.eye(rank) + cov_mat(W, W_bar) + temp * beta0 * np.outer(W_bar, W_bar))\n",
    "    var_Lambda_hyper = wishart.rvs(df = dim1 + rank, scale = var_W_hyper)\n",
    "    var_mu_hyper = mvnrnd_pre(var_mu_hyper, (dim1 + beta0) * var_Lambda_hyper)\n",
    "    \n",
    "    if dim1 * rank ** 2 > 1e+8:\n",
    "        vargin = 1\n",
    "    \n",
    "    if vargin == 0:\n",
    "        var1 = X.T\n",
    "        var2 = kr_prod(var1, var1)\n",
    "        var3 = (var2 @ tau_ind.T).reshape([rank, rank, dim1]) + var_Lambda_hyper[:, :, np.newaxis]\n",
    "        var4 = var1 @ tau_sparse_mat.T + (var_Lambda_hyper @ var_mu_hyper)[:, np.newaxis]\n",
    "        for i in range(dim1):\n",
    "            W[i, :] = mvnrnd_pre(solve(var3[:, :, i], var4[:, i]), var3[:, :, i])\n",
    "    elif vargin == 1:\n",
    "        for i in range(dim1):\n",
    "            pos0 = np.where(sparse_mat[i, :] != 0)\n",
    "            Xt = X[pos0[0], :]\n",
    "            var_mu = tau * Xt.T @ sparse_mat[i, pos0[0]] + var_Lambda_hyper @ var_mu_hyper\n",
    "            var_Lambda = tau * Xt.T @ Xt + var_Lambda_hyper\n",
    "            W[i, :] = mvnrnd_pre(solve(var_Lambda, var_mu), var_Lambda)\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPW-_ilnJDTR"
   },
   "source": [
    "Sample factor $\\boldsymbol{X}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 827,
     "status": "ok",
     "timestamp": 1615555259443,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "VerEWfC8kXts"
   },
   "outputs": [],
   "source": [
    "def sample_factor_x(tau_sparse_mat, tau_ind, W, X, beta0 = 1):\n",
    "    \"\"\"Sampling T-by-R factor matrix X and its hyperparameters (mu_x, Lambda_x).\"\"\"\n",
    "    \n",
    "    dim2, rank = X.shape\n",
    "    X_bar = np.mean(X, axis = 0)\n",
    "    temp = dim2 / (dim2 + beta0)\n",
    "    var_mu_hyper = temp * X_bar\n",
    "    var_X_hyper = inv(np.eye(rank) + cov_mat(X, X_bar) + temp * beta0 * np.outer(X_bar, X_bar))\n",
    "    var_Lambda_hyper = wishart.rvs(df = dim2 + rank, scale = var_X_hyper)\n",
    "    var_mu_hyper = mvnrnd_pre(var_mu_hyper, (dim2 + beta0) * var_Lambda_hyper)\n",
    "    \n",
    "    \n",
    "    var1 = W.T\n",
    "    var2 = kr_prod(var1, var1)\n",
    "    var3 = (var2 @ tau_ind).reshape([rank, rank, dim2]) + var_Lambda_hyper[:, :, np.newaxis]\n",
    "    var4 = var1 @ tau_sparse_mat + (var_Lambda_hyper @ var_mu_hyper)[:, np.newaxis]\n",
    "    for t in range(dim2):\n",
    "        X[t, :] = mvnrnd_pre(solve(var3[:, :, t], var4[:, t]), var3[:, :, t])\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyvD3sAPJFnM"
   },
   "source": [
    "Sampling Precision $\\tau$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 523,
     "status": "ok",
     "timestamp": 1615555260654,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "QRMwlbAdkcl2"
   },
   "outputs": [],
   "source": [
    "def sample_precision_tau(sparse_mat, mat_hat, ind):\n",
    "    var_alpha = 1e-6 + 0.5 * np.sum(ind)\n",
    "    var_beta = 1e-6 + 0.5 * np.sum(((sparse_mat - mat_hat) ** 2) * ind)\n",
    "    return np.random.gamma(var_alpha, 1 / var_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1003,
     "status": "ok",
     "timestamp": 1615555261840,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "B8QNZQHjkhYA"
   },
   "outputs": [],
   "source": [
    "def compute_mape(var, var_hat):\n",
    "    return np.sum(np.abs(var - var_hat) / var) / var.shape[0]\n",
    "\n",
    "def compute_rmse(var, var_hat):\n",
    "    return  np.sqrt(np.sum((var - var_hat) ** 2) / var.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbSPqMa8JHmx"
   },
   "source": [
    "BPMF Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 714,
     "status": "ok",
     "timestamp": 1615555263217,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "dN84t_1HkjcV"
   },
   "outputs": [],
   "source": [
    "\n",
    "def BPMF(dense_mat, sparse_mat, init, rank, burn_iter, gibbs_iter):\n",
    "    \"\"\"Bayesian Probabilistic Matrix Factorization, BPMF.\"\"\"\n",
    "    \n",
    "    dim1, dim2 = sparse_mat.shape\n",
    "    W = init[\"W\"]\n",
    "    X = init[\"X\"]\n",
    "    if np.isnan(sparse_mat).any() == False:\n",
    "        ind = sparse_mat != 0\n",
    "        pos_obs = np.where(ind)\n",
    "        pos_test = np.where((dense_mat != 0) & (sparse_mat == 0))\n",
    "    elif np.isnan(sparse_mat).any() == True:\n",
    "        pos_test = np.where((dense_mat != 0) & (np.isnan(sparse_mat)))\n",
    "        ind = ~np.isnan(sparse_mat)\n",
    "        pos_obs = np.where(ind)\n",
    "        sparse_mat[np.isnan(sparse_mat)] = 0\n",
    "    dense_test = dense_mat[pos_test]\n",
    "    del dense_mat\n",
    "    tau = 1\n",
    "    W_plus = np.zeros((dim1, rank))\n",
    "    X_plus = np.zeros((dim2, rank))\n",
    "    temp_hat = np.zeros(sparse_mat.shape)\n",
    "    show_iter = 200\n",
    "    mat_hat_plus = np.zeros(sparse_mat.shape)\n",
    "    for it in range(burn_iter + gibbs_iter):\n",
    "        tau_ind = tau * ind\n",
    "        tau_sparse_mat = tau * sparse_mat\n",
    "        W = sample_factor_w(tau_sparse_mat, tau_ind, W, X, tau)\n",
    "        X = sample_factor_x(tau_sparse_mat, tau_ind, W, X)\n",
    "        mat_hat = W @ X.T\n",
    "        tau = sample_precision_tau(sparse_mat, mat_hat, ind)\n",
    "        temp_hat += mat_hat\n",
    "        if (it + 1) % show_iter == 0 and it < burn_iter:\n",
    "            temp_hat = temp_hat / show_iter\n",
    "            print('Iter: {}'.format(it + 1))\n",
    "            print('MAPE: {:.6}'.format(compute_mape(dense_test, temp_hat[pos_test])))\n",
    "            print('RMSE: {:.6}'.format(compute_rmse(dense_test, temp_hat[pos_test])))\n",
    "            temp_hat = np.zeros(sparse_mat.shape)\n",
    "            print()\n",
    "        if it + 1 > burn_iter:\n",
    "            W_plus += W\n",
    "            X_plus += X\n",
    "            mat_hat_plus += mat_hat\n",
    "    mat_hat = mat_hat_plus / gibbs_iter\n",
    "    W = W_plus / gibbs_iter\n",
    "    X = X_plus / gibbs_iter\n",
    "    print('Imputation MAPE: {:.6}'.format(compute_mape(dense_test, mat_hat[pos_test])))\n",
    "    print('Imputation RMSE: {:.6}'.format(compute_rmse(dense_test, mat_hat[pos_test])))\n",
    "    print()\n",
    "    \n",
    "    return mat_hat, W, X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CB1AYaNpvnds"
   },
   "source": [
    "#### Non-random missing (NM) tests "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oO8q-O8fJLdY"
   },
   "source": [
    "Scenario setting:\n",
    "\n",
    "\n",
    "*   Tensor size: $43\\times 11\\times 25$ (# sensors, hours, whole time series)\n",
    "*   Non-random missing (NM)\n",
    "*   30% missing rate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 505,
     "status": "ok",
     "timestamp": 1615555298020,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "yAUs9mEJkpEO"
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "tensor = np.copy(outTensor)\n",
    "random_matrix = np.random.rand(outTensor.shape[0],outTensor.shape[1])\n",
    "random_tensor = np.random.rand(outTensor.shape[0],outTensor.shape[1], outTensor.shape[2])\n",
    "dense_mat = tensor.reshape([tensor.shape[0], tensor.shape[1] * tensor.shape[2]])\n",
    "missing_rate = 0.3\n",
    "\n",
    "## Non-random missing (NM)\n",
    "binary_tensor = np.zeros(tensor.shape)\n",
    "for i1 in range(tensor.shape[0]):\n",
    "    for i2 in range(tensor.shape[1]):\n",
    "        binary_tensor[i1, i2, :] = np.round(random_matrix[i1, i2] + 0.5 - missing_rate)\n",
    "binary_mat = binary_tensor.reshape([binary_tensor.shape[0], binary_tensor.shape[1] * binary_tensor.shape[2]])\n",
    "sparse_mat = np.multiply(dense_mat, binary_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 153771,
     "status": "ok",
     "timestamp": 1615555457430,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "Xg7cs21joKKM",
    "outputId": "52d6dd5c-1ba6-4507-ea8c-91b6b7d43aec"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 60\n",
    "init = {\"W\": 0.01 * np.random.randn(dim1, rank), \"X\": 0.01 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X = BPMF(dense_mat, sparse_mat, init, rank, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fSwVWpgm9yO"
   },
   "source": [
    "We can observe that the performance of the algorithm is worse when we assume that the data is non-random missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEliURZ0cRaP"
   },
   "source": [
    "#### Random Mising Test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1d_ZByTr9MO"
   },
   "source": [
    "Tensor size:  43×11×25  (# sensors, hours, intervals of hours time series)\n",
    "Non-random missing (NM)\n",
    "30% missing rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 507,
     "status": "ok",
     "timestamp": 1615555489791,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "2i-Au4O4Qp2a"
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "tensor = np.copy(outTensor)\n",
    "random_matrix = np.random.rand(outTensor.shape[0],outTensor.shape[1])\n",
    "random_tensor = np.random.rand(outTensor.shape[0],outTensor.shape[1], outTensor.shape[2])\n",
    "dense_mat = tensor.reshape([tensor.shape[0], tensor.shape[1] * tensor.shape[2]])\n",
    "missing_rate = 0.3\n",
    "\n",
    "## Random missing (RM)\n",
    "binary_mat = (np.round(random_tensor + 0.5 - missing_rate)\n",
    "              .reshape([random_tensor.shape[0], random_tensor.shape[1] * random_tensor.shape[2]]))\n",
    "sparse_mat = np.multiply(dense_mat, binary_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfsmSbrInU9c"
   },
   "source": [
    "\n",
    "\n",
    "*   Low rank: 80\n",
    "*   The number of burn-in iterations: 1000\n",
    "* The number of Gibbs iterations: 200\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 248834,
     "status": "ok",
     "timestamp": 1615555740270,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "x4Rn02pLcoHn",
    "outputId": "ec7f8779-556f-4979-e401-ffb513c8ea4e"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 80\n",
    "init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X = BPMF(dense_mat, sparse_mat, init, rank, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_nlUOuoKiG9"
   },
   "source": [
    "dense_mat is the original vector of data and mat_hat is the imputated vector in each position of these vector we can observe the time series distribution of a different sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 635
    },
    "executionInfo": {
     "elapsed": 983,
     "status": "ok",
     "timestamp": 1615555745413,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "cU2RDGzdhn7F",
    "outputId": "c33fcc88-85d8-4c5a-bef9-566e884adc85"
   },
   "outputs": [],
   "source": [
    "plotTs(dense_mat[12], mat_hat[12], title='Average PM2.5 filled by BPMF,when the percentage of missing values equals 30%, Sensor 31')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3O8qsGmyoITZ"
   },
   "source": [
    "We can observe that the imputated value is similar to the actual value, and the performance indicators are very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jl4-exrJoYkY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlokAHW5Ulk1"
   },
   "source": [
    "### Bayesian Temporal Matrix Factorization (**BTMF**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 700,
     "status": "ok",
     "timestamp": 1615555903717,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "_dcTfiutVU19"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv as inv\n",
    "from numpy.random import normal as normrnd\n",
    "from scipy.linalg import khatri_rao as kr_prod\n",
    "from scipy.stats import wishart\n",
    "from scipy.stats import invwishart\n",
    "from numpy.linalg import solve as solve\n",
    "from numpy.linalg import cholesky as cholesky_lower\n",
    "from scipy.linalg import cholesky as cholesky_upper\n",
    "from scipy.linalg import solve_triangular as solve_ut\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szm3RtfRrUxr"
   },
   "source": [
    "Sampling Factor Matrix $W$ and Its Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 323,
     "status": "ok",
     "timestamp": 1615555903937,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "XIAjA0SPVxxw"
   },
   "outputs": [],
   "source": [
    "\n",
    "def mvnrnd_pre(mu, Lambda):\n",
    "    src = normrnd(size = (mu.shape[0],))\n",
    "    return solve_ut(cholesky_upper(Lambda, overwrite_a = True, check_finite = False), \n",
    "                    src, lower = False, check_finite = False, overwrite_b = True) + mu\n",
    "\n",
    "def cov_mat(mat, mat_bar):\n",
    "    mat = mat - mat_bar\n",
    "    return mat.T @ mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 523,
     "status": "ok",
     "timestamp": 1615555904873,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "F2Peg6K4V0XX"
   },
   "outputs": [],
   "source": [
    "def sample_factor_w(tau_sparse_mat, tau_ind, W, X, tau, beta0 = 1, vargin = 0):\n",
    "    \"\"\"Sampling N-by-R factor matrix W and its hyperparameters (mu_w, Lambda_w).\"\"\"\n",
    "    \n",
    "    dim1, rank = W.shape\n",
    "    W_bar = np.mean(W, axis = 0)\n",
    "    temp = dim1 / (dim1 + beta0)\n",
    "    var_W_hyper = inv(np.eye(rank) + cov_mat(W, W_bar) + temp * beta0 * np.outer(W_bar, W_bar))\n",
    "    var_Lambda_hyper = wishart.rvs(df = dim1 + rank, scale = var_W_hyper)\n",
    "    var_mu_hyper = mvnrnd_pre(temp * W_bar, (dim1 + beta0) * var_Lambda_hyper)\n",
    "    \n",
    "    if dim1 * rank ** 2 > 1e+8:\n",
    "        vargin = 1\n",
    "    \n",
    "    if vargin == 0:\n",
    "        var1 = X.T\n",
    "        var2 = kr_prod(var1, var1)\n",
    "        var3 = (var2 @ tau_ind.T).reshape([rank, rank, dim1]) + var_Lambda_hyper[:, :, None]\n",
    "        var4 = var1 @ tau_sparse_mat.T + (var_Lambda_hyper @ var_mu_hyper)[:, None]\n",
    "        for i in range(dim1):\n",
    "            W[i, :] = mvnrnd_pre(solve(var3[:, :, i], var4[:, i]), var3[:, :, i])\n",
    "    elif vargin == 1:\n",
    "        for i in range(dim1):\n",
    "            pos0 = np.where(sparse_mat[i, :] != 0)\n",
    "            Xt = X[pos0[0], :]\n",
    "            var_mu = tau[i] * Xt.T @ sparse_mat[i, pos0[0]] + var_Lambda_hyper @ var_mu_hyper\n",
    "            var_Lambda = tau[i] * Xt.T @ Xt + var_Lambda_hyper\n",
    "            W[i, :] = mvnrnd_pre(solve(var_Lambda, var_mu), var_Lambda)\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GbKXfSnrbJf"
   },
   "source": [
    "Sampling VAR Coefficients $A$ and Its Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 750,
     "status": "ok",
     "timestamp": 1615555906037,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "QA-cHrlQV2d2"
   },
   "outputs": [],
   "source": [
    "def mnrnd(M, U, V):\n",
    "    \"\"\"\n",
    "    Generate matrix normal distributed random matrix.\n",
    "    M is a m-by-n matrix, U is a m-by-m matrix, and V is a n-by-n matrix.\n",
    "    \"\"\"\n",
    "    dim1, dim2 = M.shape\n",
    "    X0 = np.random.randn(dim1, dim2)\n",
    "    P = cholesky_lower(U)\n",
    "    Q = cholesky_lower(V)\n",
    "    \n",
    "    return M + P @ X0 @ Q.T\n",
    "\n",
    "def sample_var_coefficient(X, time_lags):\n",
    "    dim, rank = X.shape\n",
    "    d = time_lags.shape[0]\n",
    "    tmax = np.max(time_lags)\n",
    "    \n",
    "    Z_mat = X[tmax : dim, :]\n",
    "    Q_mat = np.zeros((dim - tmax, rank * d))\n",
    "    for k in range(d):\n",
    "        Q_mat[:, k * rank : (k + 1) * rank] = X[tmax - time_lags[k] : dim - time_lags[k], :]\n",
    "    var_Psi0 = np.eye(rank * d) + Q_mat.T @ Q_mat\n",
    "    var_Psi = inv(var_Psi0)\n",
    "    var_M = var_Psi @ Q_mat.T @ Z_mat\n",
    "    var_S = np.eye(rank) + Z_mat.T @ Z_mat - var_M.T @ var_Psi0 @ var_M\n",
    "    Sigma = invwishart.rvs(df = rank + dim - tmax, scale = var_S)\n",
    "    \n",
    "    return mnrnd(var_M, var_Psi, Sigma), Sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PI5HYTYHrdle"
   },
   "source": [
    "Sampling Factor Matrix $X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 364,
     "status": "ok",
     "timestamp": 1615555906261,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "-rPI_sZWV5Ti"
   },
   "outputs": [],
   "source": [
    "def sample_factor_x(tau_sparse_mat, tau_ind, time_lags, W, X, A, Lambda_x):\n",
    "    \"\"\"Sampling T-by-R factor matrix X.\"\"\"\n",
    "    \n",
    "    dim2, rank = X.shape\n",
    "    tmax = np.max(time_lags)\n",
    "    tmin = np.min(time_lags)\n",
    "    d = time_lags.shape[0]\n",
    "    A0 = np.dstack([A] * d)\n",
    "    for k in range(d):\n",
    "        A0[k * rank : (k + 1) * rank, :, k] = 0\n",
    "    mat0 = Lambda_x @ A.T\n",
    "    mat1 = np.einsum('kij, jt -> kit', A.reshape([d, rank, rank]), Lambda_x)\n",
    "    mat2 = np.einsum('kit, kjt -> ij', mat1, A.reshape([d, rank, rank]))\n",
    "    \n",
    "    var1 = W.T\n",
    "    var2 = kr_prod(var1, var1)\n",
    "    var3 = (var2 @ tau_ind).reshape([rank, rank, dim2]) + Lambda_x[:, :, None]\n",
    "    var4 = var1 @ tau_sparse_mat\n",
    "    for t in range(dim2):\n",
    "        Mt = np.zeros((rank, rank))\n",
    "        Nt = np.zeros(rank)\n",
    "        Qt = mat0 @ X[t - time_lags, :].reshape(rank * d)\n",
    "        index = list(range(0, d))\n",
    "        if t >= dim2 - tmax and t < dim2 - tmin:\n",
    "            index = list(np.where(t + time_lags < dim2))[0]\n",
    "        elif t < tmax:\n",
    "            Qt = np.zeros(rank)\n",
    "            index = list(np.where(t + time_lags >= tmax))[0]\n",
    "        if t < dim2 - tmin:\n",
    "            Mt = mat2.copy()\n",
    "            temp = np.zeros((rank * d, len(index)))\n",
    "            n = 0\n",
    "            for k in index:\n",
    "                temp[:, n] = X[t + time_lags[k] - time_lags, :].reshape(rank * d)\n",
    "                n += 1\n",
    "            temp0 = X[t + time_lags[index], :].T - np.einsum('ijk, ik -> jk', A0[:, :, index], temp)\n",
    "            Nt = np.einsum('kij, jk -> i', mat1[index, :, :], temp0)\n",
    "        \n",
    "        var3[:, :, t] = var3[:, :, t] + Mt\n",
    "        if t < tmax:\n",
    "            var3[:, :, t] = var3[:, :, t] - Lambda_x + np.eye(rank)\n",
    "        X[t, :] = mvnrnd_pre(solve(var3[:, :, t], var4[:, t] + Nt + Qt), var3[:, :, t])\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PYInD8ArhN4"
   },
   "source": [
    "Sampling Precision $\\tau$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 659,
     "status": "ok",
     "timestamp": 1615555907134,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "PCnnD90MWA5E"
   },
   "outputs": [],
   "source": [
    "\n",
    "def sample_precision_tau(sparse_mat, mat_hat, ind):\n",
    "    var_alpha = 1e-6 + 0.5 * np.sum(ind, axis = 1)\n",
    "    var_beta = 1e-6 + 0.5 * np.sum(((sparse_mat - mat_hat) ** 2) * ind, axis = 1)\n",
    "    return np.random.gamma(var_alpha, 1 / var_beta)\n",
    "\n",
    "def sample_precision_scalar_tau(sparse_mat, mat_hat, ind):\n",
    "    var_alpha = 1e-6 + 0.5 * np.sum(ind)\n",
    "    var_beta = 1e-6 + 0.5 * np.sum(((sparse_mat - mat_hat) ** 2) * ind)\n",
    "    return np.random.gamma(var_alpha, 1 / var_beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFL-m7RvrlA2"
   },
   "source": [
    "Prrformance indicators functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 863,
     "status": "ok",
     "timestamp": 1615555907824,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "v3HbFLoZWDOl"
   },
   "outputs": [],
   "source": [
    "def compute_mape(var, var_hat):\n",
    "    return np.sum(np.abs(var - var_hat) / var) / var.shape[0]\n",
    "\n",
    "def compute_rmse(var, var_hat):\n",
    "    return  np.sqrt(np.sum((var - var_hat) ** 2) / var.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INDT1J0CrjYL"
   },
   "source": [
    "BTMF Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 651,
     "status": "ok",
     "timestamp": 1615556992492,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "d-yvfQXkWFqH"
   },
   "outputs": [],
   "source": [
    "def BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter, option = \"factor\"):\n",
    "    \"\"\"Bayesian Temporal Matrix Factorization, BTMF.\"\"\"\n",
    "    \n",
    "    dim1, dim2 = sparse_mat.shape\n",
    "    lossMape = []\n",
    "    lossRmse = []\n",
    "    d = time_lags.shape[0]\n",
    "    W = init[\"W\"]\n",
    "    X = init[\"X\"]\n",
    "    if np.isnan(sparse_mat).any() == False:\n",
    "        ind = sparse_mat != 0\n",
    "        pos_obs = np.where(ind)\n",
    "        pos_test = np.where((dense_mat != 0) & (sparse_mat == 0))\n",
    "    elif np.isnan(sparse_mat).any() == True:\n",
    "        pos_test = np.where((dense_mat != 0) & (np.isnan(sparse_mat)))\n",
    "        ind = ~np.isnan(sparse_mat)\n",
    "        pos_obs = np.where(ind)\n",
    "        sparse_mat[np.isnan(sparse_mat)] = 0\n",
    "    dense_test = dense_mat[pos_test]\n",
    "    del dense_mat\n",
    "    tau = np.ones(dim1)\n",
    "    W_plus = np.zeros((dim1, rank))\n",
    "    X_plus = np.zeros((dim2, rank))\n",
    "    A_plus = np.zeros((rank * d, rank))\n",
    "    temp_hat = np.zeros(len(pos_test[0]))\n",
    "    show_iter = 200\n",
    "    mat_hat_plus = np.zeros((dim1, dim2))\n",
    "    for it in range(burn_iter + gibbs_iter):\n",
    "        tau_ind = tau[:, None] * ind\n",
    "        tau_sparse_mat = tau[:, None] * sparse_mat\n",
    "        W = sample_factor_w(tau_sparse_mat, tau_ind, W, X, tau)\n",
    "        A, Sigma = sample_var_coefficient(X, time_lags)\n",
    "        X = sample_factor_x(tau_sparse_mat, tau_ind, time_lags, W, X, A, inv(Sigma))\n",
    "        mat_hat = W @ X.T\n",
    "        if option == \"factor\":\n",
    "            tau = sample_precision_tau(sparse_mat, mat_hat, ind)\n",
    "        elif option == \"pca\":\n",
    "            tau = sample_precision_scalar_tau(sparse_mat, mat_hat, ind)\n",
    "            tau = tau * np.ones(dim1)\n",
    "        temp_hat += mat_hat[pos_test]\n",
    "        if (it + 1) % show_iter == 0 and it < burn_iter:\n",
    "            temp_hat = temp_hat / show_iter\n",
    "            print('Iter: {}'.format(it + 1))\n",
    "            print('MAPE: {:.6}'.format(compute_mape(dense_test, temp_hat)))\n",
    "            print('RMSE: {:.6}'.format(compute_rmse(dense_test, temp_hat)))\n",
    "            temp_hat = np.zeros(len(pos_test[0]))\n",
    "            lossMape.append(compute_mape(dense_test, temp_hat))\n",
    "            lossRmse.append(compute_rmse(dense_test, temp_hat))\n",
    "            print()\n",
    "        if it + 1 > burn_iter:\n",
    "            W_plus += W\n",
    "            X_plus += X\n",
    "            A_plus += A\n",
    "            mat_hat_plus += mat_hat\n",
    "    mat_hat = mat_hat_plus / gibbs_iter\n",
    "    W = W_plus / gibbs_iter\n",
    "    X = X_plus / gibbs_iter\n",
    "    A = A_plus / gibbs_iter\n",
    "    print('Imputation MAPE: {:.6}'.format(compute_mape(dense_test, mat_hat[:, : dim2][pos_test])))\n",
    "    print('Imputation RMSE: {:.6}'.format(compute_rmse(dense_test, mat_hat[:, : dim2][pos_test])))\n",
    "    print()\n",
    "    mat_hat[mat_hat < 0] = 0\n",
    "    \n",
    "    return mat_hat, W, X, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JwJsa-3Wi6D"
   },
   "source": [
    "#### Random Missing tests "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihRlrt-FrvEH"
   },
   "source": [
    "Tensor size:  43×11×25  (# sensors, hours, interval of discrete time series)\n",
    "Non-random missing (NM)\n",
    "30% missing rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 569,
     "status": "ok",
     "timestamp": 1615556994551,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "87V4qOxEWJW3"
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "tensor = np.copy(outTensor)\n",
    "random_matrix = np.random.rand(outTensor.shape[0],outTensor.shape[1])\n",
    "random_tensor = np.random.rand(outTensor.shape[0],outTensor.shape[1], outTensor.shape[2])\n",
    "dense_mat = tensor.reshape([tensor.shape[0], tensor.shape[1] * tensor.shape[2]])\n",
    "missing_rate = 0.3\n",
    "\n",
    "## Random missing (RM)\n",
    "binary_mat = (np.round(random_tensor + 0.5 - missing_rate)\n",
    "              .reshape([random_tensor.shape[0], random_tensor.shape[1] * random_tensor.shape[2]]))\n",
    "sparse_mat = np.multiply(dense_mat, binary_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 478661,
     "status": "ok",
     "timestamp": 1615557473470,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "-NzpENSXWv09",
    "outputId": "7691a2dd-792a-4441-e51e-f111a373eaa6"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "rank = 80\n",
    "time_lags = np.array([1, 2, 25])\n",
    "init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "mat_hat, W, X, A = BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 635
    },
    "executionInfo": {
     "elapsed": 970,
     "status": "ok",
     "timestamp": 1615557479399,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "AQAuyy7R9hor",
    "outputId": "957f82c1-e885-4551-ed4f-4bf0dac55624"
   },
   "outputs": [],
   "source": [
    "plotTs(dense_mat[25], mat_hat[25], title='Average PM2.5 filled by BTMF,when the percentage of missing values equals 25%, Sensor 31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 597,
     "status": "ok",
     "timestamp": 1615557484971,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "MMka-jQopdlJ",
    "outputId": "580b6e8d-87ad-4581-ee05-87bc79775516"
   },
   "outputs": [],
   "source": [
    "np.isnan(mat_hat).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 6333,
     "status": "ok",
     "timestamp": 1615557494525,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "VG17X0E4pijM",
    "outputId": "e6383530-49e0-43c3-99be-441c0e753dc0"
   },
   "outputs": [],
   "source": [
    "for idx in range(0, round(dense_mat.shape[0]/2)):\n",
    "  plotTs(dense_mat[idx], mat_hat[idx], title='Average PM2.5 filled by BTMF,when the percentage of missing values equals 40%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6Bp2qtRvy4g"
   },
   "source": [
    "### Linear Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 538,
     "status": "ok",
     "timestamp": 1615559427549,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "kK2uUba_w4aB"
   },
   "outputs": [],
   "source": [
    "aux_arr = aux.to_numpy()\n",
    "aux_incomplete =  np.copy(aux_arr)\n",
    "missing_raw_values = np.random.uniform(0, 1, aux_arr.shape)\n",
    "missing_mask = missing_raw_values < 0.3\n",
    "missing_mask[0] = False\n",
    "aux_incomplete[missing_mask] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 575,
     "status": "ok",
     "timestamp": 1615559428324,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "VVmgC0DDxiNd"
   },
   "outputs": [],
   "source": [
    "aux_incomplete = pd.DataFrame(aux_incomplete, columns = columnst, index= indext).reset_index()\n",
    "aux_incomplete = aux_incomplete.copy(deep=True)\n",
    "#aux_incomplete = aux_incomplete.T.drop(columns=['Date', 'Hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 784,
     "status": "ok",
     "timestamp": 1615559429384,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "7q87SseSv2_D"
   },
   "outputs": [],
   "source": [
    "LI = aux_incomplete.interpolate(method='linear')\n",
    "LI = LI.set_index('Sensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 710,
     "status": "ok",
     "timestamp": 1615559542182,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "dIcA7OJWyg8l"
   },
   "outputs": [],
   "source": [
    "LI = LI.T.reset_index().drop(columns=['Date', 'Hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 652
    },
    "executionInfo": {
     "elapsed": 1227,
     "status": "ok",
     "timestamp": 1615559565481,
     "user": {
      "displayName": "diego carreño",
      "photoUrl": "",
      "userId": "04170820117994049109"
     },
     "user_tz": -60
    },
    "id": "LdjNMcfmwfWu",
    "outputId": "b7f163fa-76b8-4a55-bde6-28f27f0f9d2a"
   },
   "outputs": [],
   "source": [
    "#sensor 31-33-34\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "plt.plot(aux.T['sensor31v2.csv'], 'r', label='Actual', linewidth=0.8)\n",
    "plt.plot(LI['sensor31v2.csv'], 'b', label='Imputed', linewidth=0.8)\n",
    "plt.title('Average PM2.5 filled by Linear interpolation,when the percentage of missing values equals 25%, Sensor 31', size=20)\n",
    "#plt.title('Original average PM2.5 with a percentaje of missing values equals 25%, Sensor 31', size=20)\n",
    "plt.xlabel('Time',size=18)\n",
    "plt.ylabel('Average PM2.5',size=18)\n",
    "#plt.legend(['Actual', 'Imputed'], fontsize=20)\n",
    "plt.legend(fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1j1FRDm2z2F"
   },
   "source": [
    "As we can see the performance of linear interpolation is not the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ro_haCSflKUy"
   },
   "source": [
    "## **Network implementation**\n",
    "\n",
    "In this section the implementation of the network is made based on real information about the physical configuration of the sensor network, thanks to the adjacency matrix of the distances between sensors. On the other hand the analysis of the network behavior is made based on a simulated signal of the measurement of the sensors in differents instants in time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRBEIaqoswxZ"
   },
   "source": [
    "### Graph construction\n",
    "In this section it is described how from real information about the location of the sensors a graph can be built"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e07tD3EuZUvr"
   },
   "source": [
    "There is information on the distance between sensors, this is between 0 and 20 with this information we proceed to the construction of the final network. Taking into account that it is presented to reach a simple representation of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pZVmjAMAaVAy"
   },
   "outputs": [],
   "source": [
    "W = pd.read_csv('data/Pune_SensorLocationDistances.csv',header=0).set_index('0')\n",
    "W.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0AJxnol61CtL"
   },
   "outputs": [],
   "source": [
    "Indices = [1,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 21,\n",
    "            22, 23, 24, 25, 26, 27, 28, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40,\n",
    "            41, 42, 43, 45, 46, 48, 50, 51, 59]\n",
    "W.columns = [int(x) for x in W.columns]\n",
    "W= W.loc[Indices,Indices]\n",
    "W.shape\n",
    "len(Indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bm5ZyqlgmRjW"
   },
   "outputs": [],
   "source": [
    "fig = px.imshow(W)\n",
    "fig.update_layout(\n",
    "    title='Adjacent matrix')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzskJT4zed2k"
   },
   "source": [
    "The following chart shows the probability distribution function (PDF) and the cumulative distribution function (CDF) of the distances in order to analyze and choose the limit to establish the weight between connections. It is observed that the average value of the connections is between 5 and 7 and there are few connections greater than 17. If we take a threshold of 5 that is, connections greater than this value would be eliminated, 50% of the existing connections would be maintained, and we focus on the sensors that have a certain closeness in space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4GDnA2eJOibg",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Histogram(x=W.to_numpy().flatten(),nbinsx=12,histnorm='probability density')])\n",
    "fig.update_layout(title = \"PDF Distances\",xaxis_title=\"Distances\")\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure(data=[go.Histogram(x=W.to_numpy().flatten(),nbinsx=12,histnorm='probability density',cumulative_enabled=True)])\n",
    "fig.update_layout(title = \"CDF Distances\",xaxis_title=\"Distances\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enTS-OdKe58a"
   },
   "source": [
    "To define the weight of an edge connecting two vertices is used the thresholded Gaussian kernel weighting function given by :\n",
    "\n",
    "\n",
    "$ W_{i,j} = \\left\\{\\begin{matrix}\n",
    "exp(- \\frac{\\left | dist(i,j) \\right |^{^2}}{2 \\theta ^{2}}) &  dist(i,j) \\leq k \\\\ \n",
    "0 & otherwise\n",
    "\\end{matrix}\\right.$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FD6pOSI1ac68"
   },
   "outputs": [],
   "source": [
    "Theta, k = 4, 6 #Choosen values of k and theta\n",
    "W_normal = Norm_W(W.copy(),Theta,k) \n",
    "A=W_normal.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y1NB6e5KmXvz",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = px.imshow(W_normal)\n",
    "fig.update_layout(title='Adjacent matrix')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLDxGZgB4d8a"
   },
   "source": [
    "The following chart shows the probability distribution function (PDF) and the cumulative distribution function (CDF) of the weighted matrix. Most of the connections are between 0 and 0.4, i.e. they have a low weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TDt8LCRt3yyI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Histogram(x=W_normal.to_numpy().flatten(),nbinsx=12,histnorm='probability density')])\n",
    "fig.update_layout(title = \"PDF Distances\",xaxis_title=\"Distances\")\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure(data=[go.Histogram(x=W_normal.to_numpy().flatten(),nbinsx=12,histnorm='probability density',cumulative_enabled=True)])\n",
    "fig.update_layout(title = \"CDF Distances\",xaxis_title=\"Distances\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFUmkCkiaq8P"
   },
   "source": [
    "The following graph shows the network with the original values of the adjacent matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hlu1pby1ap2A"
   },
   "outputs": [],
   "source": [
    "G = create_graph(W)\n",
    "plot_graph(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qkl_g66rbFeL"
   },
   "outputs": [],
   "source": [
    "W_aux=W.copy()\n",
    "column=W_aux.columns.to_list()\n",
    "W_aux[column] = W_aux[column].where(~(W_aux[column]>10),other=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qH568Rk6bQQ9"
   },
   "source": [
    "The following chart shows the W chart limiting the network connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ovMeb-aQbK8X"
   },
   "outputs": [],
   "source": [
    "G = create_graph(W_aux)\n",
    "plot_graph(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTXL7_Vazq9K"
   },
   "source": [
    "A algotirm is applied to reduce the number of neighbors per vertex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8G2W_F760NES"
   },
   "outputs": [],
   "source": [
    "A_neigh = Neighboors(A,5)\n",
    "np.fill_diagonal(A_neigh, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.imshow(A_neigh)\n",
    "fig.update_layout(title='Adjacent matrix')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqyFkYEv5Q2D"
   },
   "source": [
    "In the following graph the final network is presented taking into account the smoothing of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fL1sxPc-boj1"
   },
   "outputs": [],
   "source": [
    "G = create_graph(A_neigh)\n",
    "plot_graph(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o2mNj2eQ8DmU"
   },
   "outputs": [],
   "source": [
    "# Theta, k = 4, 6 #Choosen values of k and theta\n",
    "# W_normal = Norm_W(W.copy(),Theta,k) \n",
    "A=A_neigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mf58ZZCuw8ap"
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "dt = [('len', float)]\n",
    "A = A.view(dt)\n",
    "\n",
    "G = nx.from_numpy_matrix(A)\n",
    "pos = nx.spring_layout(G, k=0.5*1/np.sqrt(len(G.nodes())), iterations=50)\n",
    "figure = pyplot.figure(figsize=(20, 10))\n",
    "nx.draw(G,pos=nx.spring_layout(G,pos=pos),with_labels = True)\n",
    "xyz = np.array([pos[v] for v in sorted(G)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBvU68Jo6RN-"
   },
   "source": [
    "### Graph organization\n",
    "\n",
    "In this section two methods are presented to reorganize the graph from the information given in the weight matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYZGkgW1F-YC"
   },
   "source": [
    "#### Spectral clusterin\n",
    "This method makes use of the unsupervised spectral clustering algorithm in order to identify nodes that have close distances, and group them in the same set. This in order to obtain a reorganization of the network.\n",
    "[Source](https://towardsdatascience.com/unsupervised-machine-learning-spectral-clustering-algorithm-implemented-from-scratch-in-python-205c87271045)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1bKlV71-5Md"
   },
   "source": [
    "Initially Adjacency matrix W, Degree matrix D and the Laplacian matrix to obtain the eigenvalues and eigenvectors of the L matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NhArG-xl83oW"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from pygsp import graphs\n",
    "\n",
    "A_s=A_neigh\n",
    "graph = graphs.Graph(A_s)\n",
    "graph.set_coordinates(xyz)\n",
    "graph.compute_fourier_basis()\n",
    "e = graph.e\n",
    "v = graph.U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00mO_xHZ9AkR"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[18, 6])\n",
    "ax1 = plt.subplot(221)\n",
    "plt.plot(e)\n",
    "ax1.title.set_text('eigenvalues')\n",
    "\n",
    "i = np.where(e < 20)[0]\n",
    "ax2 = plt.subplot(222)\n",
    "plt.plot(v[:, i[0]])\n",
    "#ax2.title.set_text('first eigenvector with eigenvalue')\n",
    "ax3 = plt.subplot(223)\n",
    "plt.plot(v[:, i[1]])\n",
    "ax3.title.set_text('second eigenvector with eigenvalue close to 0')\n",
    "ax4 = plt.subplot(224)\n",
    "plt.plot(v[:, i[2]])\n",
    "ax4.title.set_text('third eigenvector with eigenvalue close to 0')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RzNWGvF49E4Y"
   },
   "outputs": [],
   "source": [
    "def project_and_transpose(eigenvals, eigenvcts, num_ev):\n",
    "    \"\"\"Select the eigenvectors corresponding to the first \n",
    "    (sorted) num_ev eigenvalues as columns in a data frame.\n",
    "    \"\"\"\n",
    "    eigenvals_sorted_indices = np.argsort(eigenvals)\n",
    "    indices = eigenvals_sorted_indices[: num_ev]\n",
    "\n",
    "    proj_df = pd.DataFrame(eigenvcts[:, indices.squeeze()])\n",
    "    proj_df.columns = ['v_' + str(c) for c in proj_df.columns]\n",
    "    return proj_df\n",
    "\n",
    "proj_df= project_and_transpose(e,v,10) #Taking the first 10 eigenvalues to make the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xz0V6fysASla"
   },
   "source": [
    "The K-Means algorithm is applied and the inertia is calculated to choose the best possible number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cJ5zwIlv9Kro"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "inertias = []\n",
    "\n",
    "k_candidates = range(1, 20)\n",
    "\n",
    "for k in k_candidates:\n",
    "    k_means = KMeans(random_state=42, n_clusters=k)\n",
    "    k_means.fit(proj_df)\n",
    "    inertias.append(k_means.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-C0HXW4lBUdS"
   },
   "source": [
    "From the following graph K=10 is chosen because it has an inertia very close to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sMovMI_09OLn"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style('darkgrid', {'axes.facecolor': '.9'})\n",
    "sns.set_palette(palette='deep')\n",
    "sns_c = sns.color_palette(palette='deep')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.scatterplot(x=k_candidates, y = inertias, s=80, ax=ax)\n",
    "sns.lineplot(x=k_candidates, y = inertias, alpha=0.5, ax=ax)\n",
    "ax.set(title='Inertia K-Means', ylabel='inertia', xlabel='k');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UxExS9hT9QW9"
   },
   "outputs": [],
   "source": [
    "def run_k_means(df, n_clusters):\n",
    "    \"\"\"K-means clustering.\"\"\"\n",
    "    k_means = KMeans(random_state=25, n_clusters=n_clusters)\n",
    "    k_means.fit(df)\n",
    "    cluster = k_means.predict(df)\n",
    "    return cluster\n",
    "\n",
    "def spectral_clustering(eigenvals, eigenvcts, n_clusters):\n",
    "    \"\"\"Spectral Clustering Algorithm.\"\"\"\n",
    "    proj_df = project_and_transpose(eigenvals, eigenvcts, n_clusters)\n",
    "    cluster = run_k_means(proj_df, proj_df.columns.size)\n",
    "    return cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGTUPvtcBlFb"
   },
   "source": [
    "The algorithm is executed for a number of clusters equal to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nAm6xqr79ZLs"
   },
   "outputs": [],
   "source": [
    "Index_cluster= spectral_clustering(e,v,8)\n",
    "index = np.argsort(Index_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsGo0ZzdBzUB"
   },
   "source": [
    "In the following graph you can see the result of the clustering, where each color represents a different group. It is observed that most of the nodes that are physically close are grouped in the same set. This information is used to reorganize the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0FEZe2wd9ng0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dt = [('len', float)]\n",
    "A_s = A_s.view(dt)\n",
    "G = nx.from_numpy_matrix(A_s)\n",
    "pos = nx.spring_layout(G, k=0.5*1/np.sqrt(len(G.nodes())), iterations=50)\n",
    "figure = pyplot.figure(figsize=(20, 10))\n",
    "nx.draw(G,pos=nx.spring_layout(G,pos=pos),with_labels = True,node_color = Index_cluster, cmap = 'hsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfrzjHCnMqay"
   },
   "outputs": [],
   "source": [
    "SortW = Sort=pd.DataFrame(data = A_neigh).iloc[index,index]\n",
    "A_s=SortW.values\n",
    "Grap_s=A_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Umah3EhLmeCY"
   },
   "outputs": [],
   "source": [
    "fig = px.imshow(Grap_s)\n",
    "fig.update_layout(\n",
    "    title='Adjacent matrix')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mm7R8IcQ9tBt"
   },
   "outputs": [],
   "source": [
    "dt = [('len', float)]\n",
    "A_s = A_s.view(dt)\n",
    "\n",
    "G = nx.from_numpy_matrix(A_s)\n",
    "pos = nx.spring_layout(G, k=0.5*1/np.sqrt(len(G.nodes())), iterations=50)\n",
    "figure = pyplot.figure(figsize=(20, 10))\n",
    "nx.draw(G,pos=nx.spring_layout(G,pos=pos),with_labels = True)\n",
    "xyz = np.array([pos[v] for v in sorted(G)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzVGOJ9WGqwU"
   },
   "source": [
    "#### Bandwidth Reduction\n",
    "\n",
    "The reverse Cuthill–McKee algorithm is an algorithm to permute a sparse matrix that has a symmetric sparsity pattern into a band matrix form with a small bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3hA2TWMU3PME"
   },
   "outputs": [],
   "source": [
    "Theta, k = 4, 6\n",
    "W_normal = Norm_W(W.copy(),Theta,k) \n",
    "A=W_normal.values\n",
    "A_neigh = Neighboors(A,5)\n",
    "np.fill_diagonal(A_neigh, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aXXk0yd5k2a_"
   },
   "outputs": [],
   "source": [
    "#Function\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import reverse_cuthill_mckee\n",
    "\n",
    "\n",
    "def ReOrderGraph(A):\n",
    "    csrMatrix = csr_matrix(A)\n",
    "    perm = reverse_cuthill_mckee(csrMatrix)\n",
    "    A_reorder = csrMatrix[perm, :][:, perm].toarray()\n",
    "    return A_reorder,perm\n",
    "\n",
    "def Graph_Nodes(A):\n",
    "    dt = [('len', float)]\n",
    "    A = A.view(dt)\n",
    "    G = nx.from_numpy_matrix(A)\n",
    "    pos = nx.spring_layout(G, k=0.5*1/np.sqrt(len(G.nodes())), iterations=50)\n",
    "    figure = pyplot.figure(figsize=(20, 10))\n",
    "    nx.draw(G,pos=nx.spring_layout(G,pos=pos),with_labels = True)\n",
    "    xyz = np.array([pos[v] for v in sorted(G)])\n",
    "    return xyz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "La0JtbQNHRzA"
   },
   "source": [
    "In order to apply the algorithm, the \"Scipy\" library is used, which returns the permutation array that orders a sparse CSR or CSC matrix in Reverse-Cuthill McKee ordering. [source 2](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csgraph.reverse_cuthill_mckee.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5u_xS8ADk4f1"
   },
   "outputs": [],
   "source": [
    "A_reorder,perm = ReOrderGraph(A_neigh)\n",
    "xyz = Graph_Nodes(A_reorder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fn6Tz8VImktX"
   },
   "outputs": [],
   "source": [
    "fig = px.imshow(A_reorder)\n",
    "fig.update_layout(\n",
    "    title='Adjacent matrix')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckmwAN1Ik68H"
   },
   "outputs": [],
   "source": [
    "graph = graphs.Graph(A_reorder)\n",
    "graph.set_coordinates(xyz)\n",
    "graph.compute_fourier_basis()\n",
    "\n",
    "Index_cluster = spectral_clustering(graph.e,graph.U,8)\n",
    "plot_graph(graph, Index_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8WTpY2lKCE7"
   },
   "source": [
    "It is observed that with both methods a similar result is obtained, however due to simplicity the Cuthill-McKee band reduction method is chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GFtC_iYFWBzE"
   },
   "outputs": [],
   "source": [
    "graph = graphs.Graph(A_reorder)\n",
    "graph.set_coordinates(xyz)\n",
    "graph.compute_fourier_basis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KP0Jc6wwR-Ph"
   },
   "source": [
    "## **Simulated Drift Implementation**\n",
    "\n",
    "\n",
    "In this section We will define sinusoidal signals for all the nodes of the network to be analyzed, presenting in the vertex 0 a drift behavior from a certain instant of time. We will then analyze different characteristics of the network using the Graph Fourier Transform (GFT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w66DglcMR-Ph"
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams[\"image.cmap\"] = 'viridis'\n",
    "\n",
    "# Constants\n",
    "SPACE_GRAPH_ORDER, SPACE_KERNEL_SCALE = graph.N, 1 \n",
    "TIME_GRAPH_ORDER, TIME_KERNEL_SCALE = 50, 10\n",
    "dT = 10.0/TIME_GRAPH_ORDER # Time sample\n",
    "LEVEL_NOISE, BREAK_TIME, STD = 0.3, 20, 3 # LN, brownian Motion (BM) start-point and SD in BM\n",
    "VERTEX_D = 4 # Drift vertex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByzsXuvjvf6K"
   },
   "source": [
    "### Network in time and space\n",
    "\n",
    "In the following graph, the two graphs to be analyzed are presented. On the one hand, we have the network of sensors obtained in the previous session, and on the other, we have a path graph which represents the time, in this case 50 different moments. Small groups of nodes are made, in order to simulate similar segmented behavior in the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KG0mSruaR-Ph"
   },
   "outputs": [],
   "source": [
    "# Space-time graph definition\n",
    "groups = np.array([0]*21 + [1]*22)\n",
    "space_time_graph = [graph, create_path_graph(TIME_GRAPH_ORDER)]\n",
    "\n",
    "plot_graph(space_time_graph[0],groups)\n",
    "plot_graph(space_time_graph[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-CyHfcceD9Au"
   },
   "outputs": [],
   "source": [
    "kernel_anima(space_time_graph, (SPACE_GRAPH_ORDER//2,TIME_GRAPH_ORDER//2), \n",
    "             windows_kernels = [SPACE_KERNEL_SCALE, TIME_KERNEL_SCALE])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-ENXLKBTgNk"
   },
   "source": [
    "### Signal definition\n",
    "Now we create a function that defines the signal to be implemented on the space network, based on different space/time eigenvalues.\n",
    "\n",
    "Let's see how each signal looks in all the vertex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMpt4UfpD9Av"
   },
   "outputs": [],
   "source": [
    "sgroups = np.array([5]*21 + [34]*22)\n",
    "tgroups = [6]*(TIME_GRAPH_ORDER//3) + [20]*(TIME_GRAPH_ORDER//3) + [38]*(TIME_GRAPH_ORDER - 2*(TIME_GRAPH_ORDER//3))\n",
    "\n",
    "space_time_graph = [graph, create_path_graph(TIME_GRAPH_ORDER)]\n",
    "time_series = time_space_signal_gen(space_time_graph, sgroups, tgroups, ln = LEVEL_NOISE, normalize = True)\n",
    "brownian(time_series[VERTEX_D,BREAK_TIME - 1], TIME_GRAPH_ORDER - BREAK_TIME, dT, STD, out = time_series[VERTEX_D,BREAK_TIME:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SKZ_Kydpw364",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "# Create traces\n",
    "fig = go.Figure()\n",
    "for i in range(len(time_series)):\n",
    "    fig.add_trace(go.Scatter(x=list(range(len(time_series[i]))), \n",
    "                             y=time_series[i], mode='lines', name='Node'+str(i)))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Experimental signals\",\n",
    "    xaxis_title=\"Time\",\n",
    "    yaxis_title=\"Signal value\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-EWeNi_TgNm"
   },
   "source": [
    "The next graph shows the signal inside the space-time graph in a given time. For example, at time instant 3, it is observed that the network is divided into two sections because the signal strength is inverse for the groups created, that is, one group of nodes has a high value while the other has a low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ifoBfo8pTgNn",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "signal_graph_anima(space_time_graph, time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgEvAmalIkGe"
   },
   "source": [
    "### GFT in time and space\n",
    "\n",
    "In this section we present functions to calculate the graph fourier transform in time and space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--UUf0ReR-Pi"
   },
   "source": [
    "To check that the desired signal was processed correctly, the signal on a particular node and its Graph Fourier Transform (GFT) can be displayed with the following code section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Qqo-WoLvf6L"
   },
   "outputs": [],
   "source": [
    "gft_signal_anima(space_time_graph[1], time_series, is_graph_space = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kH2G9HstuGuN"
   },
   "source": [
    "From the graph above it can be seen that the signal at node 0 (where the drift occurs) most of its energy is concentrated in the low frequencies, in other words in the eigenvalues close to 0. For the other nodes, there are higher frequencies, being these on the seventh eigenvalue.\n",
    "\n",
    "This behavior is explained having in mind that after the drift, the signal in the 0 vertex presents a constant decrease, while in the rest of sensors the sinusoidal signal continues its periosity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VeGVjFUivf6L"
   },
   "outputs": [],
   "source": [
    "gft_signal_anima(space_time_graph[0], time_series, is_graph_space = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUyKei82R-Pi"
   },
   "source": [
    "From the graph above, it is observed that in general, the frequency components in time are found in greater magnitude at low levels, however they present in higher frequencies, because the signal in a moment of time varies in the network in general. After the drift in certain instants, it is observed greater power in high frequencies / eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CewWTdTbJEXt"
   },
   "source": [
    "### Spectrogram for disjoint GFT\n",
    "\n",
    "To have a more detailed understanding, we will first analyze the graphs separately. The following function allows to display the spectrogram of each network by selecting a specific time (for the spatial graph) or a specific vertex (for the temporal graph), depending on the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hhtE92n2vf6L"
   },
   "outputs": [],
   "source": [
    "spectogram_anima(space_time_graph[0], time_series, SKS = SPACE_KERNEL_SCALE, \n",
    "                 is_graph_space = True, limits = [0.0, 0.5], lamdba_lim = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HMqSLVUtvf6L",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spectogram_anima(space_time_graph[1], time_series, SKS = TIME_KERNEL_SCALE, \n",
    "                 is_graph_space = False, limits = None, lamdba_lim = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PkrhdnFR-Pi"
   },
   "source": [
    "* **Spatial-graph:** It is important to consider that the same signal is passed through a certain group of vertices $s(v,t_{i})$. It is observed that before the drift there is high power at low frequency in these vertices, after the drift higher frequency components are observed with high power values, corresponding in turn to the \"group\".\n",
    "\n",
    "* **Time-graph:** Focusing on the signal from only one particular node $s(v_{i},t)$, it is easy to determine which vertex is faulty, since the respective spectrogram differs greatly from the other vertices, even though the signal of all should be the same. For the vertex 0, it is observed that before the drift (instant 20) the frequency component is high and after it is reduced to eigenvalues close to zero, behavior that is explained because after the drift the signal decreases progressively "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Mj_ZEwfR-Pi"
   },
   "source": [
    "### Spectrogram for joint GFT (JFT)\n",
    "Now we will enter to observe the properties of the space-time joint spectrogram. This is possible thanks to the following function. Since the joint spectrogram is in principle a two-dimensional array for each possible combination in space and time, we have as a result an array in $\\mathbb{R}^4$, being useful to indicate a specific vertex and instant, looking for interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q8rb37B7vf6L",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kernels, joint_spectogram = JFT_anima(space_time_graph, time_series, [SPACE_KERNEL_SCALE, TIME_KERNEL_SCALE],\n",
    "                                     lspace_lim = None, ltime_lim = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vp_3lQpR-Pi"
   },
   "source": [
    "Taking into account that the joint spectrogram estimates different spectrograms, each of them centered on the combination ($v_i$, $t_j$), the analysis of the results should be done for each possible combination, arriving at the following statements:\n",
    "\n",
    "1.  As explained above, the process occurs at \"low frequencies\" for any possible event, in time, which directly affects the joint spectrogram, being only possible to see non-zero magnitudes in the upper left region (low orders of eigenvalues in time).\n",
    "2.  The signal $s(v_i,t)$ has more frequency components compared to the signal $s(v,t_i)$, being possible to observe in the spectrogram for any combination. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9-Dlo74-dDY"
   },
   "source": [
    "### Drift detection\n",
    "\n",
    "In order to detect the drift concept, we will use one technique to detect outliers. For this, we will perform the subtraction of sub-spectrograms centered on the pair $(v_i, t_j)$ and plot the result, trying to find the most changing sub-spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rYD2zEUw-dDZ"
   },
   "outputs": [],
   "source": [
    "SPACE_WINDOW, TIME_WINDOW = 0, 2\n",
    "filename = './data/drift_joint_sim.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "edhao0NEKbG9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dist_matrix = plot_dist_matrix(joint_spectogram.copy(), SPACE_WINDOW, TIME_WINDOW, filename = filename, \n",
    "                               norm_each_sg = False, overwrite = False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJaC6P3ZD9A0"
   },
   "outputs": [],
   "source": [
    "dist_matrix = plot_dist_matrix(joint_spectogram.copy(), SPACE_WINDOW, TIME_WINDOW, filename = filename, \n",
    "                               norm_each_sg = False, overwrite = False,\n",
    "                               vlist = range(VERTEX_D-1,VERTEX_D+2),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKYlYoTrDvvW"
   },
   "source": [
    "## **Real signal analysis**\n",
    "\n",
    "By observing a first view of the behavior of a signal that presents drift in one of its nodes, in order to understand its spectral representation, we can begin to analyze the signals captured by the sensor network. To do this, we must take a reading of the gas to be analyzed: PM, which is a measure of how polluted the air is at a location. We will then build a matrix containing the signal from each sensor per hour, from August 28 to October 28, 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhE4LRdaDvvX"
   },
   "source": [
    "### Data read\n",
    "Let's start with the reading of the signals, contained in `air-polution-sensor/data/final_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ATEk66BfDvvX"
   },
   "outputs": [],
   "source": [
    "time_sensors_series = pd.read_csv('./data/final_data.csv', header = None)\n",
    "print(\"[INFO] New index: \", perm)\n",
    "time_sensors_series = time_sensors_series.values[perm,:]\n",
    "print(\"[INFO] Time series shape: \", time_sensors_series.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use only a specific portion of the signal, where we previously located the drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_sensors_series_windows = time_sensors_series[:,250:290].copy()\n",
    "time_sensors_series_windows[29,10:] *= 2.0 # Drift highest in magnitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-gxc1l2YgLPI",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import datetime\n",
    "\n",
    "# Create traces\n",
    "fig = go.Figure()\n",
    "base_t = datetime.datetime(2018, 8, 12)\n",
    "t = np.array([base_t + datetime.timedelta(hours = i) for i in range(time_sensors_series.shape[1])])\n",
    "t = t[250:290] # 250:290\n",
    "\n",
    "for i in range(len(time_sensors_series_windows)):\n",
    "    fig.add_trace(go.Scatter(x = t, y = time_sensors_series_windows[i],\n",
    "                    mode='lines',\n",
    "                    name='Node'+str(i)))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Study window\",\n",
    "    xaxis_title=\"Time instant\",\n",
    "    yaxis_title=\"PM 2\"\n",
    "\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDnDcjSQgANI"
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams[\"image.cmap\"] = 'viridis'\n",
    "\n",
    "# Constants\n",
    "SPACE_GRAPH_ORDER, SPACE_KERNEL_SCALE = graph.N, 1\n",
    "TIME_GRAPH_ORDER, TIME_KERNEL_SCALE = time_sensors_series_windows.shape[1], 60\n",
    "\n",
    "space_time_graph = [graph, create_path_graph(TIME_GRAPH_ORDER)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_anima(space_time_graph, (29,TIME_GRAPH_ORDER//2), \n",
    "             windows_kernels = [SPACE_KERNEL_SCALE, TIME_KERNEL_SCALE])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7MM2pF8AP9m"
   },
   "source": [
    "### DFT analysis\n",
    "Let's how to the graph behaves in the frequency domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkUw-dvkdayM"
   },
   "source": [
    "#### Spatial-graph spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OFmCTxQaekdH",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spectogram_anima(space_time_graph[0], time_sensors_series_windows, SKS = SPACE_KERNEL_SCALE, is_graph_space = True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55zgmIVbffYV"
   },
   "source": [
    "#### Time-graph spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skf5jcyPetJx"
   },
   "outputs": [],
   "source": [
    "spectogram_anima(space_time_graph[1], time_sensors_series_windows, SKS = TIME_KERNEL_SCALE, is_graph_space = False, \n",
    "                 limits = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joint spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7TpoFGQeexDM",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kernels, joint_spectogram = JFT_anima(space_time_graph, time_sensors_series_windows, [SPACE_KERNEL_SCALE, TIME_KERNEL_SCALE],\n",
    "                                     lspace_lim = None, ltime_lim = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXXMsJ5J-dDn"
   },
   "source": [
    "### Drift detection\n",
    "\n",
    "In order to detect the drift, we want to obtained the distance matrix per group and in general. Let's looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aVmbqCLO-dDo"
   },
   "outputs": [],
   "source": [
    "SPACE_WINDOW, TIME_WINDOW = 0, 2\n",
    "filename = './data/drift_joint_real.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aVmbqCLO-dDo"
   },
   "outputs": [],
   "source": [
    "dist_matrix = plot_dist_matrix(joint_spectogram.copy(), SPACE_WINDOW, TIME_WINDOW, filename = filename, \n",
    "                               norm_each_sg = False, overwrite = False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(Index_cluster)\n",
    "for index in np.unique(Index_cluster):\n",
    "    print(\"[INFO] Dist_matrix cluster \", index)\n",
    "    dist_matrix = plot_dist_matrix(joint_spectogram.copy(), SPACE_WINDOW, TIME_WINDOW, filename = filename, \n",
    "                               norm_each_sg = False, overwrite = False, vlist = np.where(Index_cluster == index)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = plot_dist_matrix(joint_spectogram.copy(), SPACE_WINDOW, TIME_WINDOW, filename = filename, \n",
    "                               norm_each_sg = False, overwrite = False, vlist = [27,28,29,30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then to localize the drift vertex, let's try to find a common cluster group, that save the behaviors of drift vertex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = dist_matrix_estimation(joint_spectogram, 0, 3, filename = filename, vlist = np.where(Index_cluster == 7)[0])\n",
    "axis = dist_matrix.index.to_series().str.split(',', expand = True)\n",
    "dist_matrix['vertex'] = axis[0].str.replace('(','').astype(int)\n",
    "dist_matrix['time'] = axis[1].str.replace(')','').astype(int)\n",
    "\n",
    "dist_matrix['vertex'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "X = dist_matrix.copy()\n",
    "\n",
    "lowest_bic = [np.infty, None]\n",
    "bic = []\n",
    "n_components_range = range(2, 11)\n",
    "cv_types = ['spherical', 'tied', 'diag', 'full']\n",
    "for cv_type in cv_types:\n",
    "#     print(\"[INFO] CV_type = {}\".format(cv_type))\n",
    "    for n_components in tqdm.tqdm(n_components_range):\n",
    "        # Fit a Gaussian mixture with EM\n",
    "        gmm = GaussianMixture(n_components = n_components, covariance_type = cv_type)\n",
    "        gmm.fit(X)\n",
    "        bic.append(gmm.bic(X))\n",
    "        if bic[-1] < lowest_bic[0]:\n",
    "            lowest_bic = [bic[-1], cv_type]\n",
    "            best_gmm = gmm\n",
    "\n",
    "# Graph\n",
    "X['labels'] = best_gmm.predict(X).astype(str)\n",
    "fig = px.scatter(X, x = 'time', y = 'vertex', color = 'labels')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the instances of vertex 29 where the drift is presented are grouped into a single group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHUePVT20CNs"
   },
   "source": [
    "## ***Anexos: GitHub-Colab connection***\n",
    "Here, some commands to upload/save the github respository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Teasr5DEz_xw"
   },
   "outputs": [],
   "source": [
    "''' Function definitions'''\n",
    "# Git pull\n",
    "def git_pull(repo_pwd, show_current_branch = False, make_commit = False): # Only for colab space work\n",
    "    global user_git, email_git\n",
    "    import sys\n",
    "    IN_COLAB = 'google.colab' in sys.modules\n",
    "    if IN_COLAB:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/gdrive')\n",
    "\n",
    "        %cd \"$repo_pwd\"\n",
    "        # !git config --list\n",
    "        if show_current_branch: \n",
    "            !git branch \n",
    "        if make_commit:\n",
    "            if \"user_git\" not in globals(): user_git = input(\"User github?: \")\n",
    "            if \"email_git\" not in globals(): email_git = input(\"Email github?: \") \n",
    "            !git config --global user.email $email_git\n",
    "            !git config --global user.name $user_git\n",
    "            !git commit -am \"Updating in colab\"\n",
    "        !git pull\n",
    "        !git status\n",
    "    else:\n",
    "        print(\"[INFO] You are not in collaboration, nothing has been done.\")\n",
    "\n",
    "# Git push\n",
    "def git_push(repo_pwd): # Only for colab space work\n",
    "    global user_git, email_git\n",
    "    import sys\n",
    "    IN_COLAB = 'google.colab' in sys.modules\n",
    "    if IN_COLAB:\n",
    "        from google.colab import drive\n",
    "        import getpass\n",
    "        drive.mount('/content/gdrive')\n",
    "\n",
    "        %cd \"$repo_pwd\"\n",
    "        if \"user_git\" not in globals(): user_git = input(\"User github?: \")\n",
    "        if \"email_git\" not in globals(): email_git = input(\"Email github?: \")\n",
    "\n",
    "        # Password login\n",
    "        try: \n",
    "            pwd_git = getpass.getpass(prompt='{} github password: '.format(user_git)) \n",
    "        except Exception as error: \n",
    "            print('ERROR', error) \n",
    "\n",
    "        # Upload from every where\n",
    "        origin_git = !git config --get remote.origin.url\n",
    "        origin_git = origin_git[0].replace(\"https://\",\"https://{}:{}@\".format(user_git,pwd_git))\n",
    "\n",
    "        !git config --global user.email \"$email_git\"\n",
    "        !git config --global user.name \"$user_git\"\n",
    "        !git status\n",
    "\n",
    "        x = \" \"\n",
    "        while x.lower() != \"y\" and x.lower() != \"n\": x = input(\"Continue?...[y/n]: \")\n",
    "\n",
    "        if x.lower() == \"y\":\n",
    "            com_message = input(\"Enter the commit message: \")\n",
    "            !git add .\n",
    "            !git commit -am \"$com_message\"\n",
    "            !git push \"$origin\"\n",
    "            !git status\n",
    "    else:\n",
    "        print(\"[INFO] You are not in collaboration, nothing has been done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNJ68hztrBN_"
   },
   "source": [
    "In order to execute the functions, please unlock the respective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tNPGLIe1q37C"
   },
   "outputs": [],
   "source": [
    "# git_pull(repo_pwd, show_current_branch = False, make_commit = True)\n",
    "# git_push(repo_pwd)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "CD8jG37W1sA7"
   ],
   "name": "sensors_drift.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": "1",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "236px",
    "left": "99px",
    "top": "180px",
    "width": "212px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
